{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AnshikaSingh33/voiceCloning/blob/main/voiceCloning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xA8trt8-UVKF"
      },
      "outputs": [],
      "source": [
        "!pip install -q --upgrade ffmpeg-python git+https://github.com/openai/whisper.git soundfile datasets yt-dlp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iYzRIMsyvIYB"
      },
      "outputs": [],
      "source": [
        "!pip install numpy==2.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ywbl0XK5UVyc"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import subprocess\n",
        "import uuid\n",
        "import torch\n",
        "import whisper\n",
        "import glob\n",
        "from pathlib import Path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IuB8rqv1UhQg"
      },
      "outputs": [],
      "source": [
        "\n",
        "MODEL_SIZE = \"turbo\"\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "\n",
        "AUDIO_DIR = Path(\"/content/audio\")\n",
        "AUDIO_DIR.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "print(f\" Using {MODEL_SIZE} model on {DEVICE}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jbfgb2OfWP5F"
      },
      "outputs": [],
      "source": [
        "YOUTUBE_URLS = [\n",
        "    \"http://youtube.com/watch?v=br8hkcZ1YV0\",\n",
        "]\n",
        "for url in YOUTUBE_URLS:\n",
        "    try:\n",
        "\n",
        "        process = subprocess.run(\n",
        "            [\"yt-dlp\", \"--print\", \"%(title)s\", url],\n",
        "            capture_output=True, text=True, check=True\n",
        "        )\n",
        "        title = process.stdout.strip()\n",
        "\n",
        "        expected_filename = AUDIO_DIR / f\"{title}.m4a\"\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Warning: Could not determine expected filename for {url}: {e}\")\n",
        "\n",
        "        print(f\"* Downloading {url} (without checking for existing file)\")\n",
        "        subprocess.run([\n",
        "            \"yt-dlp\",\n",
        "            \"-q\", \"-x\",\n",
        "            \"--audio-format\", \"m4a\",\n",
        "            \"-o\", str(AUDIO_DIR / \"%(title)s.%(ext)s\"),\n",
        "            url\n",
        "        ], check=True)\n",
        "\n",
        "        continue\n",
        "\n",
        "\n",
        "    if expected_filename.exists():\n",
        "        print(f\"\\n Skipping download for {url} (already exists as {expected_filename.name})\")\n",
        "    else:\n",
        "        print(f\"\\n Downloading {url}\")\n",
        "\n",
        "        subprocess.run([\n",
        "            \"yt-dlp\",\n",
        "            \"-q\", \"-x\",\n",
        "            \"--audio-format\", \"m4a\",\n",
        "            \"-o\", str(expected_filename),\n",
        "            url\n",
        "        ], check=True)\n",
        "        continue;\n",
        "\n",
        "\n",
        "if expected_filename.exists():\n",
        "    print(f\" Skipping download for {url} (already exists as {expected_filename.name})\")\n",
        "else:\n",
        "    print(f\" Downloading {url}\")\n",
        "\n",
        "    subprocess.run([\n",
        "        \"yt-dlp\",\n",
        "        \"-q\",\n",
        "        \"-x\",\n",
        "        \"--audio-format\", \"m4a\",\n",
        "        \"-o\", str(AUDIO_DIR / \"%(title)s.%(ext)s\"),\n",
        "        url\n",
        "    ], check=True)\n",
        "\n",
        "\n",
        "print(f\"\\nAll YouTube audio saved to {AUDIO_DIR}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tE_HY23NYmaS"
      },
      "outputs": [],
      "source": [
        "\n",
        "from pathlib import Path\n",
        "import subprocess\n",
        "from IPython.display import Audio, display\n",
        "\n",
        "\n",
        "CLEANED_AUDIO_DIR = Path(\"/content/audio_clean\")\n",
        "CLEANED_AUDIO_DIR.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "\n",
        "\n",
        "def clean_one(src: Path) -> Path:\n",
        "    \"\"\"\n",
        "    Applies a series of FFmpeg filters to clean and normalize an audio file.\n",
        "\n",
        "    Args:\n",
        "        src: Path to the source audio file.\n",
        "\n",
        "    Returns:\n",
        "        Path to the cleaned output audio file in CLEANED_AUDIO_DIR.\n",
        "    \"\"\"\n",
        "\n",
        "    out = CLEANED_AUDIO_DIR / f\"{src.stem}_clean.wav\"\n",
        "\n",
        "\n",
        "    filter_chain = (\n",
        "        \"highpass=f=80,\"\n",
        "        \"afftdn,\"\n",
        "        \"loudnorm=I=-16:LRA=11:TP=-1.5,\"\n",
        "        \"dynaudnorm=f=200,\"\n",
        "        \"apad=pad_dur=0.1\"\n",
        "    )\n",
        "\n",
        "\n",
        "    subprocess.run([\n",
        "        'ffmpeg',\n",
        "        '-loglevel', 'error',\n",
        "        '-y',\n",
        "        '-i', str(src),\n",
        "        '-af', filter_chain,\n",
        "        str(out)\n",
        "    ], check=True)\n",
        "\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9SNPfIH4Zv5I"
      },
      "outputs": [],
      "source": [
        "import subprocess\n",
        "from pathlib import Path\n",
        "from IPython.display import Audio, display\n",
        "\n",
        "\n",
        "\n",
        "AUDIO_DIR = Path(\"/content/audio\")\n",
        "\n",
        "\n",
        "audio_paths = sorted(list(AUDIO_DIR.glob('*.*')))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def get_duration_sec(filepath: Path) -> float:\n",
        "    \"\"\"Gets the duration of an audio file in seconds using ffprobe.\"\"\"\n",
        "    cmd = [\n",
        "        \"ffprobe\", \"-v\", \"error\", \"-show_entries\", \"format=duration\",\n",
        "        \"-of\", \"default=noprint_wrappers=1:nokey=1\", str(filepath)\n",
        "    ]\n",
        "    result = subprocess.run(cmd, capture_output=True, text=True, check=True)\n",
        "    return float(result.stdout.strip())\n",
        "\n",
        "print(\"\\n---  Comparing Original vs. Cleaned Audio ---\")\n",
        "if audio_paths:\n",
        "    first_original_audio_path = audio_paths[0]\n",
        "\n",
        "    print(\"\\nOriginal audio (first 10 seconds):\")\n",
        "    temp_original_clip_path = Path(\"/content/temp_original_clip_10s.wav\")\n",
        "\n",
        "    subprocess.run([\n",
        "        \"ffmpeg\", \"-loglevel\", \"error\", \"-y\",\n",
        "        \"-i\", str(first_original_audio_path),\n",
        "        \"-ss\", \"0\", \"-t\", \"10\",\n",
        "        str(temp_original_clip_path)\n",
        "    ], check=True)\n",
        "\n",
        "    display(Audio(str(temp_original_clip_path)))\n",
        "    original_clip_duration = get_duration_sec(temp_original_clip_path)\n",
        "    print(f\"Duration: {original_clip_duration:.3f} seconds\")\n",
        "\n",
        "    print(\"\\nCleaning the 10-second clip...\")\n",
        "    temp_cleaned_clip_path = clean_one(temp_original_clip_path)\n",
        "\n",
        "    print(\"\\nCleaned audio (first 10 seconds):\")\n",
        "    display(Audio(str(temp_cleaned_clip_path)))\n",
        "    cleaned_clip_duration = get_duration_sec(temp_cleaned_clip_path)\n",
        "    print(f\"Duration: {cleaned_clip_duration:.3f} seconds\")\n",
        "else:\n",
        "    print(\" No audio files found to compare.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-9I2BP3LcDX8"
      },
      "outputs": [],
      "source": [
        "\n",
        "cleaned_paths = []\n",
        "total_clean_sec = 0\n",
        "\n",
        "\n",
        "for p in audio_paths:\n",
        "\n",
        "    cleaned_file = CLEANED_AUDIO_DIR / f\"{p.stem}_clean.wav\"\n",
        "\n",
        "\n",
        "    if cleaned_file.exists():\n",
        "        print(f\"‚è© Skipping cleaning for {p.name} (already exists)\")\n",
        "        cleaned_paths.append(cleaned_file)\n",
        "        dur = get_duration_sec(cleaned_file)\n",
        "        total_clean_sec += dur\n",
        "    else:\n",
        "\n",
        "        print(f\"üßº Cleaning {p.name}...\", end=\"\")\n",
        "        cleaned = clean_one(p)\n",
        "        cleaned_paths.append(cleaned)\n",
        "\n",
        "\n",
        "        dur = get_duration_sec(cleaned)\n",
        "        total_clean_sec += dur\n",
        "        print(f\" done ({dur/60:.2f} min)\")\n",
        "\n",
        "\n",
        "print(f\"\\n All cleaned files are in {CLEANED_AUDIO_DIR}\")\n",
        "print(f\"Total cleaned duration: {total_clean_sec/60:.2f} min ({total_clean_sec/3600:.2f} h)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XXxVWDdBiK5V"
      },
      "outputs": [],
      "source": [
        "\n",
        "cleaned_paths = sorted(list(CLEANED_AUDIO_DIR.glob(\"*.wav\")))\n",
        "\n",
        "print(f\"\\n Found {len(cleaned_paths)} cleaned audio files.\")\n",
        "\n",
        "total_sec = 0\n",
        "print(\"-\" * 60)\n",
        "for p in cleaned_paths:\n",
        "    dur_sec = get_duration_sec(p)\n",
        "    total_sec += dur_sec\n",
        "\n",
        "    print(f\"‚Ä¢ {p.name:<40} {dur_sec/60:6.2f} min\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "\n",
        "print(f\"\\nTotal duration: {total_sec/60:.2f} min ({total_sec/3600:.2f} h)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0L6hfaF4juvQ"
      },
      "outputs": [],
      "source": [
        "import whisper\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "\n",
        "model = whisper.load_model(MODEL_SIZE, device=DEVICE)\n",
        "\n",
        "\n",
        "TRANSCRIPTS_DIR = Path(\"/content/transcripts\")\n",
        "TRANSCRIPTS_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "\n",
        "for audio_path in cleaned_paths:\n",
        "    fname = audio_path.stem\n",
        "    out_json = TRANSCRIPTS_DIR / f\"{fname}.json\"\n",
        "\n",
        "\n",
        "    if out_json.exists():\n",
        "        print(f\"‚è© Skipping transcription for {fname} (already exists)\")\n",
        "        continue\n",
        "\n",
        "\n",
        "    print(f\" Transcribing {fname}...\")\n",
        "    result = model.transcribe(\n",
        "        str(audio_path),\n",
        "        word_timestamps=True,\n",
        "        fp16=(DEVICE == \"cuda\"),\n",
        "        verbose=False\n",
        "    )\n",
        "\n",
        "\n",
        "    with out_json.open(\"w\") as f:\n",
        "        json.dump(result, f, indent=2)\n",
        "\n",
        "print(f\"\\n Done! All transcripts saved in {TRANSCRIPTS_DIR}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d80E3_CtlZAs"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import login, HfApi\n",
        "from huggingface_hub.utils import LocalTokenNotFoundError\n",
        "\n",
        "\n",
        "try:\n",
        "    HfApi().whoami()\n",
        "    print(\" Already logged into Hugging Face.\")\n",
        "except LocalTokenNotFoundError:\n",
        "    print(\"Not logged into Hugging Face. Logging in...\")\n",
        "    login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SYpxljFSoZe_"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "import subprocess\n",
        "import uuid\n",
        "from pathlib import Path\n",
        "\n",
        "import datasets\n",
        "\n",
        "\n",
        "HF_ORG = \"Anshika33\"\n",
        "REPO_NAME = \"Anshi-voice-dataset\"\n",
        "MAX_SEC = 30\n",
        "\n",
        "\n",
        "SAMPLING_RATE = 24_000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XTMXKoC8pOnN"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import os\n",
        "import subprocess\n",
        "import uuid\n",
        "from pathlib import Path\n",
        "import datasets\n",
        "import nltk\n",
        "from nltk.tokenize import PunktSentenceTokenizer\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "TRANSCRIPTS_DIR = Path(\"/content/transcripts\")\n",
        "CLIPS_ROOT = Path(\"/content/clips\")\n",
        "CLIPS_ROOT.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "\n",
        "nltk.download('punkt', quiet=True)\n",
        "tokenizer = PunktSentenceTokenizer()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def chunk_one(audio_path: Path, json_path: Path):\n",
        "    \"\"\"Processes one audio/transcript pair to create clips.\"\"\"\n",
        "\n",
        "\n",
        "    with open(json_path) as f:\n",
        "        data = json.load(f)\n",
        "    word_meta, full_text, char_pos = [], [], 0\n",
        "    for seg in data[\"segments\"]:\n",
        "        for w in seg[\"words\"]:\n",
        "            tok = w[\"word\"].strip()\n",
        "            if not tok: continue\n",
        "            start_char, end_char = char_pos, char_pos + len(tok)\n",
        "            word_meta.append({\"char0\": start_char, \"char1\": end_char, \"t0\": w[\"start\"], \"t1\": w[\"end\"]})\n",
        "            full_text.extend([tok, \" \"])\n",
        "            char_pos = end_char + 1\n",
        "    full_text = \"\".join(full_text).rstrip()\n",
        "\n",
        "\n",
        "    sent_spans = list(tokenizer.span_tokenize(full_text))\n",
        "    sentences, w_idx = [], 0\n",
        "    for c0, c1 in sent_spans:\n",
        "        first_w = w_idx\n",
        "        while first_w < len(word_meta) and word_meta[first_w][\"char1\"] <= c0:\n",
        "            first_w += 1\n",
        "        last_w = first_w\n",
        "        while last_w < len(word_meta) and word_meta[last_w][\"char0\"] < c1:\n",
        "            last_w += 1\n",
        "        if first_w == last_w: continue\n",
        "        s_time = word_meta[first_w][\"t0\"]\n",
        "        e_time = word_meta[last_w - 1][\"t1\"]\n",
        "        text = full_text[c0:c1]\n",
        "        sentences.append({\"start\": s_time, \"end\": e_time, \"text\": text})\n",
        "        w_idx = last_w\n",
        "\n",
        "\n",
        "    clip_rows, bundle = [], None\n",
        "\n",
        "    def flush(b):\n",
        "        \"\"\"Helper function to process a bundle and create a clip.\"\"\"\n",
        "        if b is None: return\n",
        "        st, et, tx, out_dir = b\n",
        "        out_dir.mkdir(exist_ok=True, parents=True)\n",
        "        clip_path = out_dir / f\"clip_{uuid.uuid4().hex}.wav\"\n",
        "\n",
        "        subprocess.run([\n",
        "            \"ffmpeg\", \"-loglevel\", \"error\", \"-y\",\n",
        "            \"-i\", str(audio_path),\n",
        "            \"-ss\", str(st), \"-to\", str(et),\n",
        "            \"-ar\", str(SAMPLING_RATE), \"-ac\", \"1\",\n",
        "            str(clip_path)\n",
        "        ], check=True)\n",
        "\n",
        "        clip_rows.append({\n",
        "            \"audio\": str(clip_path),\n",
        "            \"text\": tx.strip(),\n",
        "            \"source\": \"0\"\n",
        "        })\n",
        "\n",
        "    out_dir = CLIPS_ROOT / audio_path.stem\n",
        "    for s in sentences:\n",
        "        st, et, tx = s[\"start\"], s[\"end\"], s[\"text\"]\n",
        "        if (et - st) > MAX_SEC:\n",
        "            continue\n",
        "\n",
        "        if bundle is None:\n",
        "            bundle = [st, et, tx, out_dir]\n",
        "            continue\n",
        "\n",
        "        b_st, b_et, b_tx, _ = bundle\n",
        "        if (et - b_st) <= MAX_SEC:\n",
        "\n",
        "            bundle = [b_st, et, b_tx + \" \" + tx, out_dir]\n",
        "        else:\n",
        "\n",
        "            flush(bundle)\n",
        "            bundle = [st, et, tx, out_dir]\n",
        "\n",
        "    flush(bundle)\n",
        "    return clip_rows\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "rows = []\n",
        "for audio_path in cleaned_paths:\n",
        "    stem = audio_path.stem\n",
        "    json_path = TRANSCRIPTS_DIR / f\"{stem}.json\"\n",
        "\n",
        "    if not json_path.exists():\n",
        "        print(f\"‚è© {stem} ‚Üí skipping (no transcript)\")\n",
        "        continue\n",
        "\n",
        "    print(f\"üß© Chunking {stem}...\")\n",
        "    clip_rows = chunk_one(audio_path, json_path)\n",
        "    rows.extend(clip_rows)\n",
        "    print(f\"  ‚Üí Created {len(clip_rows)} clips\")\n",
        "\n",
        "print(f\"\\n Created {len(rows)} total clips across {len(cleaned_paths)} source files.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VbeDP3yyC7CO"
      },
      "outputs": [],
      "source": [
        "print(cleaned_paths)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M-AI8KTGv9z8"
      },
      "outputs": [],
      "source": [
        "print(rows)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NFzTeg0Hnler"
      },
      "outputs": [],
      "source": [
        "import datasets\n",
        "from datasets import Dataset\n",
        "\n",
        "ds = Dataset.from_list(rows)\n",
        "\n",
        "ds = ds.cast_column(\"audio\", datasets.Audio(sampling_rate=SAMPLING_RATE))\n",
        "\n",
        "repo_id = f\"{HF_ORG}/{REPO_NAME}\"\n",
        "\n",
        "print(f\" Pushing dataset to {repo_id}...\")\n",
        "ds.push_to_hub(repo_id, private=False)\n",
        "\n",
        "print(\"\\n Done! Your dataset is live on the Hub.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MSIoo5pRtRGm"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "import os\n",
        "\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "\n",
        "    !pip install unsloth tensorboard\n",
        "else:\n",
        "\n",
        "    !pip install -no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl==0.15.2 triton cut_cross_entropy unsloth_zoo ten\n",
        "    !pip install sentencepiece protobuf \"datasets>=3.4.1\" huggingface_hub hf_transfer\n",
        "    !pip install -no-deps unsloth\n",
        "    !pip install transformers==4.52.3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nNQyz21hxlfA",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "\n",
        "!pip install --force-reinstall transformers==4.52.3\n",
        "!pip install --force-reinstall unsloth[colab-new]@git+https://github.com/unslothai/unsloth.git\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TlzJ6sHUyYXR"
      },
      "outputs": [],
      "source": [
        "\n",
        "from unsloth import FastModel\n",
        "from transformers import CsmForConditionalGeneration\n",
        "import torch\n",
        "\n",
        "model_name = \"unsloth/csm-1b\"\n",
        "\n",
        "model, processor = FastModel.from_pretrained(\n",
        "    model_name = model_name,\n",
        "    max_seq_length = 2048,\n",
        "    dtype = None,\n",
        "    auto_model = CsmForConditionalGeneration,\n",
        "    load_in_4bit = True,\n",
        "    full_finetuning = True\n",
        ")\n",
        "print(\"completed successfully\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lP6Do8Ci0IGg"
      },
      "outputs": [],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "\n",
        "\n",
        "\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 32,\n",
        "    target_modules = [\n",
        "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
        "    ],\n",
        "    lora_alpha = 16,\n",
        "    lora_dropout = 0,\n",
        "    bias = \"none\",\n",
        "    use_gradient_checkpointing = \"unsloth\",\n",
        "    random_state = 3407,\n",
        "    use_rslora = True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LMIgMxV--wbj"
      },
      "outputs": [],
      "source": [
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ur52XKuW9AlD"
      },
      "outputs": [],
      "source": [
        "model.print_trainable_parameters()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V-jRKzN--RCU"
      },
      "outputs": [],
      "source": [
        "\n",
        "ft_dataset = \"Anshika33/Anshi-voice-dataset\"\n",
        "sampling_rate = 24_000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xfkKjokN_ffQ"
      },
      "outputs": [],
      "source": [
        "\n",
        "import math\n",
        "from datasets import load_dataset, Audio, DatasetDict\n",
        "from transformers import AutoProcessor\n",
        "\n",
        "\n",
        "processor = AutoProcessor.from_pretrained(\"unsloth/csm-1b\")\n",
        "raw_ds = load_dataset(ft_dataset, split=\"train\")\n",
        "\n",
        "print(f'Dataset loaded with features: {raw_ds.features[\"audio\"]}')\n",
        "\n",
        "\n",
        "speaker_key = \"source\"\n",
        "if speaker_key not in raw_ds.column_names:\n",
        "    print('Unsloth: No speaker key found. Adding a default \"source\" column.')\n",
        "\n",
        "    new_column = [\"0\"] * len(raw_ds)\n",
        "    raw_ds = raw_ds.add_column(speaker_key, new_column)\n",
        "\n",
        "\n",
        "\n",
        "raw_ds = raw_ds.cast_column(\"audio\", Audio(sampling_rate=sampling_rate))\n",
        "\n",
        "\n",
        "total_rows = len(raw_ds)\n",
        "\n",
        "eval_rows = min(30, max(1, math.ceil(0.10 * total_rows)))\n",
        "\n",
        "split: DatasetDict = raw_ds.train_test_split(\n",
        "    test_size=eval_rows,\n",
        "    shuffle=True,\n",
        "    seed=42,\n",
        ")\n",
        "\n",
        "raw_train_ds, raw_eval_ds = split[\"train\"], split[\"test\"]\n",
        "\n",
        "print(f\"Train split: {len(raw_train_ds):>5} rows\")\n",
        "print(f\"Eval split:  {len(raw_eval_ds):>5} rows (requested {eval_rows})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-2VNa9lEBi9Y"
      },
      "outputs": [],
      "source": [
        "\n",
        "max_text_length = max(len(x) for x in raw_ds[\"text\"])\n",
        "max_audio_length = max(len(x[\"array\"]) for x in raw_ds[\"audio\"])\n",
        "\n",
        "print(f\"Maximum text length in the dataset: {max_text_length}\")\n",
        "print(f\"Maximum audio length in the dataset: {max_audio_length}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dwk1l_cFCjp4"
      },
      "outputs": [],
      "source": [
        "def preprocess_example(example):\n",
        "    \"\"\"\n",
        "    Takes a single data example, formats it, processes it, and validates the output.\n",
        "    \"\"\"\n",
        "\n",
        "    conversation = [\n",
        "        {\n",
        "            \"role\": str(example[\"source\"]),\n",
        "            \"content\": [\n",
        "                {\"type\": \"text\", \"text\": example[\"text\"]},\n",
        "                {\"type\": \"audio\", \"audio\": example[\"audio\"][\"array\"]},\n",
        "            ]\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    try:\n",
        "\n",
        "        model_inputs = processor.apply_chat_template(\n",
        "            conversation,\n",
        "            tokenize=True,\n",
        "            padding=\"max_length\",\n",
        "            max_length=max_audio_length,\n",
        "            return_tensors=\"pt\",\n",
        "            return_dict=True,\n",
        "        )\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing example with text '{example['text'][:50]}...': {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "    if \"input_ids\" in model_inputs:\n",
        "\n",
        "        model_inputs[\"labels\"] = model_inputs[\"input_ids\"].clone()\n",
        "\n",
        "\n",
        "        if \"attention_mask\" in model_inputs:\n",
        "\n",
        "            model_inputs[\"labels\"][model_inputs[\"attention_mask\"] == 0] = -100\n",
        "\n",
        "\n",
        "\n",
        "    available_keys = list(model_inputs.keys())\n",
        "    print(f\"Available keys: {available_keys}\")\n",
        "\n",
        "\n",
        "    expected_keys = [\"input_ids\", \"attention_mask\", \"labels\"]\n",
        "\n",
        "    if \"input_values\" in available_keys:\n",
        "        expected_keys.append(\"input_values\")\n",
        "    if \"audio_features\" in available_keys:\n",
        "        expected_keys.append(\"audio_features\")\n",
        "    if \"pixel_values\" in available_keys:\n",
        "        expected_keys.append(\"pixel_values\")\n",
        "\n",
        "    processed_example = {}\n",
        "\n",
        "    for key in expected_keys:\n",
        "        if key not in model_inputs:\n",
        "            if key == \"labels\":\n",
        "\n",
        "                print(f\"Error: Could not create labels from input_ids\")\n",
        "                return None\n",
        "            else:\n",
        "                print(f\"Warning: Expected key '{key}' not found in processor output.\")\n",
        "\n",
        "                continue\n",
        "\n",
        "\n",
        "        value = model_inputs[key][0]\n",
        "        processed_example[key] = value\n",
        "\n",
        "\n",
        "    if not all(isinstance(v, torch.Tensor) for v in processed_example.values()):\n",
        "        print(f\"Error: Not all values are tensors. Keys: {list(processed_example.keys())}\")\n",
        "        return None\n",
        "\n",
        "    return processed_example\n",
        "\n",
        "\n",
        "def preprocess_example_flexible(example):\n",
        "    \"\"\"\n",
        "    More flexible version that adapts to whatever keys the processor actually returns.\n",
        "    \"\"\"\n",
        "\n",
        "    conversation = [\n",
        "        {\n",
        "            \"role\": str(example[\"source\"]),\n",
        "            \"content\": [\n",
        "                {\"type\": \"text\", \"text\": example[\"text\"]},\n",
        "                {\"type\": \"audio\", \"audio\": example[\"audio\"][\"array\"]},\n",
        "            ]\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    try:\n",
        "\n",
        "        model_inputs = processor.apply_chat_template(\n",
        "            conversation,\n",
        "            tokenize=True,\n",
        "            padding=\"max_length\",\n",
        "            max_length=max_audio_length,\n",
        "            return_tensors=\"pt\",\n",
        "            return_dict=True,\n",
        "        )\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing example with text '{example['text'][:50]}...': {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "    if \"labels\" not in model_inputs and \"input_ids\" in model_inputs:\n",
        "        model_inputs[\"labels\"] = model_inputs[\"input_ids\"].clone()\n",
        "\n",
        "\n",
        "        if \"attention_mask\" in model_inputs:\n",
        "            model_inputs[\"labels\"][model_inputs[\"attention_mask\"] == 0] = -100\n",
        "\n",
        "\n",
        "    processed_example = {}\n",
        "    for key, value in model_inputs.items():\n",
        "        if isinstance(value, torch.Tensor) and value.dim() > 0:\n",
        "\n",
        "            if value.shape[0] == 1:\n",
        "                processed_example[key] = value[0]\n",
        "            else:\n",
        "                processed_example[key] = value\n",
        "        else:\n",
        "            processed_example[key] = value\n",
        "\n",
        "\n",
        "    if \"input_ids\" not in processed_example or \"labels\" not in processed_example:\n",
        "        print(f\"Error: Missing essential keys. Available: {list(processed_example.keys())}\")\n",
        "        return None\n",
        "\n",
        "    return processed_example\n",
        "\n",
        "\n",
        "def debug_preprocess_example(example):\n",
        "    \"\"\"\n",
        "    Debug version to understand what the processor returns.\n",
        "    \"\"\"\n",
        "    conversation = [\n",
        "        {\n",
        "            \"role\": str(example[\"source\"]),\n",
        "            \"content\": [\n",
        "                {\"type\": \"text\", \"text\": example[\"text\"]},\n",
        "                {\"type\": \"audio\", \"audio\": example[\"audio\"][\"array\"]},\n",
        "            ]\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    try:\n",
        "        model_inputs = processor.apply_chat_template(\n",
        "            conversation,\n",
        "            tokenize=True,\n",
        "            padding=\"max_length\",\n",
        "            max_length=max_audio_length,\n",
        "            return_tensors=\"pt\",\n",
        "            return_dict=True,\n",
        "        )\n",
        "\n",
        "        print(\"=\"*50)\n",
        "        print(\"Processor output keys and shapes:\")\n",
        "        for key, value in model_inputs.items():\n",
        "            if isinstance(value, torch.Tensor):\n",
        "                print(f\"  {key}: {value.shape} ({value.dtype})\")\n",
        "            else:\n",
        "                print(f\"  {key}: {type(value)} - {value}\")\n",
        "        print(\"=\"*50)\n",
        "\n",
        "        return model_inputs\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in debug preprocessing: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "print(\"Testing debug preprocessing on first example...\")\n",
        "first_example = raw_train_ds[0]\n",
        "debug_result = debug_preprocess_example(first_example)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_1l-1620Y-pd"
      },
      "outputs": [],
      "source": [
        "\n",
        "processed_train_ds = raw_train_ds.map(\n",
        "    preprocess_example_flexible,\n",
        "    remove_columns=raw_train_ds.column_names,\n",
        "    desc=\"Preprocessing train set\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eJlKzVN9kFCn"
      },
      "outputs": [],
      "source": [
        "print(processed_train_ds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dxC8In0EmE64"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "\n",
        "run_name = model_name.split(\"/\")[-1] + '-lora-ft' + time.strftime(\"_%Y%m%d_%H%M%S\")\n",
        "print(run_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-kIj-ab3mfUa"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "from unsloth import is_bfloat16_supported\n",
        "\n",
        "trainer = Trainer(\n",
        "    model = model,\n",
        "    train_dataset = processed_train_ds,\n",
        "    eval_dataset = processed_eval_ds,\n",
        "    args = TrainingArguments(\n",
        "        per_device_train_batch_size = 2,\n",
        "        gradient_accumulation_steps = 8,\n",
        "        warmup_steps = 5,\n",
        "        num_train_epochs = 3,\n",
        "        learning_rate = 2e-4,\n",
        "        fp16 = not is_bfloat16_supported(),\n",
        "        bf16 = is_bfloat16_supported(),\n",
        "        logging_steps = 1,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"constant\",\n",
        "        seed = 3407,\n",
        "        output_dir = \"outputs\",\n",
        "\n",
        "        eval_strategy = \"steps\",\n",
        "        eval_steps = 10,\n",
        "\n",
        "        report_to = \"tensorboard\",\n",
        "        logging_dir = f\"logs/{run_name}\",\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xB4C1easm9HT"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "\n",
        "gpu_stats = torch.cuda.get_device_properties(0)\n",
        "\n",
        "\n",
        "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024**3, 3)\n",
        "max_memory = round(gpu_stats.total_memory / 1024**3, 3)\n",
        "\n",
        "\n",
        "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
        "print(f\"{start_gpu_memory} GB of memory reserved.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zabo_9VSnpr0"
      },
      "outputs": [],
      "source": [
        "\n",
        "%load_ext tensorboard\n",
        "\n",
        "\n",
        "%tensorboard --logdir logs/csm-1b-lora-ft_20250829_205230"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hE7PAiksoExD"
      },
      "outputs": [],
      "source": [
        "\n",
        "import torch\n",
        "import gc\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "\n",
        "import os\n",
        "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
        "\n",
        "\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results/minimal\",\n",
        "    per_device_train_batch_size=1,\n",
        "    gradient_accumulation_steps=16,\n",
        "    gradient_checkpointing=True,\n",
        "    fp16=True,\n",
        "    dataloader_pin_memory=False,\n",
        "    num_train_epochs=1,\n",
        "    logging_steps=10,\n",
        "    save_steps=1000,\n",
        "\n",
        "    remove_unused_columns=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HVn2ew8AuDnR"
      },
      "outputs": [],
      "source": [
        "\n",
        "!pip uninstall -y unsloth transformers accelerate bitsandbytes peft trl\n",
        "\n",
        "\n",
        "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tKejcue8opMq"
      },
      "outputs": [],
      "source": [
        "trainer_stats=trainer.train()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}